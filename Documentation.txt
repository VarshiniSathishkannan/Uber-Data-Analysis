Sample data 
https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet

Download the files and place in home folder of gcloud shell - import.sh

Upload the Raw data to GCS bucket - copy from home folder to gcs bucket

gsutil cp input/* gs://uber-data-analytics-raw/

Data modelling - lucid chart
https://lucid.app/lucidchart/5906fd18-09b5-4489-9c23-a4dbf66d728b/edit?viewport_loc=-412%2C-77%2C2455%2C1033%2C0_0&invitationId=inv_0db1a269-c038-46ba-a9d2-21f43340b970

Convert Flat file to Dimensional modelling

Model databases.jpeg

To connect to GCS bucket through local spark using service account

Inorder to use GCS bucket, we have to add the GCS bucket connector jar to the jars directory of 
spark home folder

If we are using dataproc cluster, then no need to specify any connectors

using spark dataframe for transformation 

transformation.ipynb

Data preparation is done ie fact and dimension tables

now we have to do it in the ETL tool (Automate and schedule) ie mage 

on compute engine on GCP

pip install mage-ai

mage start uberAnalysis 

this will start 

 http://localhost:6789 Mage will start in this port 
 serving project /home/neeleshmgr/uberAnalysis

 In GCP -  VM instance - we have to use external IP address and before that we have to create a firewall
 rule to open this port 

 mage is similar to airflow, We can create pipelines and schedule it. It is very simple to use 

 It is not able to use large dataset since it is running on local setup on a single VM

 I am going to use, Cloud composer - Managed Airflow instance 



